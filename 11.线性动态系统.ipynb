{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近似推断/精确推断，其中近似推断重的随即推断中最著名的即为MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.线性动态系统(Kalman Filter)  \n",
     "线性动态系统,Linear Gaussian--Kalman Filter;非线性动态系统，Non-Linear,Non-Gaussian      \n",
     "线性动态系统的表达：z_t=Az_{t-1}+B+\\epsilon x_t=Cz_t+D+\\delta,\\epsilon~N(0,Q),\\delta~N(0,R)，也可以表达为P(z_t|z_{t-1})~N(Az_{t-1}+B,Q),P(x_t|z_t)~N(Cz_t+D,R),初始状态z_1~N(\\mu_1,\\sum_1),参数\\theta=(A,B,C,D,Q,R,\\mu_1,\\sum_1)   \n",
     "滤波问题要参考前向算法的思想，P(z_t|x_1...x_t)正比于P(x_1....x_t,z_t)=P(x_t|x_1,x_2...x_{t-1},z_t)=P(x_t|z_t)P(x_1...x_{t-1},z_t)=P(x_t|z_t)P(z_t|x_1...x_{t-1})(一个预测问题)P(x_1,..x_{t-1})   \n",
     "P(z_t|x_1...x_{t-1})=\\int_z_{t-1} P(z_t,z_{t-1}|x_1...x_{t-1})dz_{t-1}=\\int_z_{t-1} P(z_t|z_{t-1},x_1...x_{t-1})P(z_{t-1}|x_1...x_{t-1}dz_{t-1}),得到递推式   \n",
     "步骤：t=1，P(z_1|x_1),update；t=2，P(z_2|x_1),prediction;t=2,P(z_2|x_1,x_2),P(z_3|x_1,x_2)...   \n",
     "因为正态分布假设，根据性质可以获得封闭解,x~N(\\mu,\\Lambda^{-1})精度矩阵-方差矩阵的逆,P(y|x)=N(y|Ax+b,L^{-1}),P(y)=N(y|A\\mu+b,L^{-1}+A\\Lambda^{-1}A^T)   \n",
     "step1,prediction:N(\\mu_t*,\\sum_t*)=\\int N(z_t|Az_{t+1}+B,Q)N(\\mu_{t-1},\\sum_{t-1})   \\mu_t*=A\\mu_{t-1}+B,\\sum*=Q+A\\sum_{t-1}A^T--step2 update N(\\mu_t,\\sum_t) 正比于N(X_t|Cz_t+D,R)N(\\mu_t*,\\sum_t*)",
     "最后p(z_t|x_t)正比于P(x_t|z_t)P(z_t)"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.HMM-Evaluation问题  \n",
    "利用\\lambda求P(O|\\lambda)",
     "P(O|\\lambda)=\\sum P(I,O|\\lambda)=\\sum P(O|I,\\lambda)P(I|\\lambda),复杂度为O(N^T)    \n",
     "前向算法：\\alpha_t(i)=P(o_1,...o_t,i_t=q_i|\\lambda),\\alpha_T(i)=P(O,i_t=q_i|\\lambda)   \n",
     "P(O|\\lambda)=\\sum_{i=1}^N P(O,i_t=q_i|\\lambda)=\\sum \\alpha_T(i)",
     "从观测序列向后推\\alpha_{t+1}(j)=P(o_1,...o_t,o_{t+1},i_{t+1}=q_j|\\lambda)=\\sum P(o...,i_{t+1},i_t)=\\sum P(O_{t+1}|i_{t+1}=q_j) P(...))(无后效性假设)，把i_{t+1}提出来=\\sum_{i=1}^N b_j(O_{t+1})a_{ij}\\alpha_t(i)，即为前向递归公式",
     "后向算法：引入\\beta_t(i)=P(o_{t+1},...,o_T|i_t=q_i,\\lambda),从后往前推，\\beta_1(i)=P(O_2,...,O_T|i_1=q_i,\\lambda)",
     "P(O|\\lambda)=P(o_1,...o_T|\\lambda)=\\sum P(o_1,...o_T|i_1=q_i)P(i_1=q_i)(->\\pi_i)=\\sum b_i(o_1)\\beta_1(i)",
     "\\beta_t(i)=\\sum b_j(O_{t+1})a_{ij}\\beta_{t+1}(j)",
     "\\beta_T->\\beta_1->P(O|\\lambda)，精度矩阵-方差矩阵的逆"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.HMM-Learning问题 \n",
    "使用EM算法，\\theta^{t+1}=argmax \\int_z log p(x,z|\\theta)p(z|x,\\theta^{t})dz,x是观测数据，z是隐变量，\\theta是参数     \n",
     "Q(\\lambda,\\lambda^(t))=\\sum(隐变量) log P(O,I|\\lambda)P(O,I|\\lambda^t)",
     "\\pi^{t+1}=argmax \\sum [log \\pi_i P(O,i_1=q_i|\\lambda^t)] s.t.\\sum \\pi_i=1",
     "\\pi_i^t=\\frac{P(O,i_1=q_i|\\lambda^t)}{P(O|\\lambda^t)}",
     "A,B的计算的思想与计算\\pi的思想相同，PS：这个就是Baum Welch算法，但其实就是EM"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.HMM-Decoding问题 \n",
    "\\hat{I}=argmax_I P(I|I,\\lambda),可以理解为一个动态规划问题,Viterbi算法,   \n",
     "\\delta_t(i)=max P(o_1...o_t,i_1...i_t=q_i),接下来需要寻找递归式   \n",
     "\\delta_{t+1}(j)=max P(o_1,o_2,...o_{t+1},i_1,...i_{t+1}=q_j)=max \\delta_t(i)a_{ij}b_j(O_{t+1})  \n",
     "\\phi_{t+1}(j)=argmax_{1<=i<=N} \\delta_t(i)a_{ij},寻找从哪来的，记录最短路径序列，进而排成一个最短路径"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五.HMM \n",
    "HMM是一个混合模型并增加时间轴，与GMM进行类比，可以把HMM理解为GMM增加时间，HMM对于发射矩阵没有特定要求，也可以为概率分布，只需要i为离散   \n",
     "动态模型-state space model状态空间模型，针对解决两大问题，1）learning 问题，通过数据学习参数；2）inference：推断后验概率P(z|x)，数据与数据之间可能存在在关联，作用：1）decoding，给定观测序列，求隐状态序列P(z_1...z_t|x_1,x_2,x_t);2)prob of evidence:P(x|\\theta)=P(x_1,x_2...x_T|\\theta) 使用前向或后向算法;3)filtering:P(z_t|x_1,...x_t),滤波问题，因为拿到了更多的历史数据，可以过滤掉更多的噪声->在线学习，P(z_t|x_{1:t})正比于\\alpha_t；4)smoothing P(z_t|x_1...x_T)->offline，P(x_{1:T},z_t)=P(x_{t+1,T}|x_{1:t},z_t)P(x_{1:t},z_t)正比于P(x_{t+1:T},z_t)=\\alpha_t\\beta_t，转换时，使用概率图中条件独立的思想,前向后向算法；5)prediction，P(z_{.}/x_{.}|x_1,x_2...x_t)，P(Z_{t+1}|x_{1:t})=\\sum P(z_{t+1},z_t|x_{1:t})=\\sum P(z_{t+1}|z_t,x_{t+1})P(z_t|x_{1:t})P(z_{t+1}|z_t)这就是filtering问题了，P(x_{t+1}|x_{1:t})=\\sum P(x_{t+1},z_{t+1}|x{1:t})=\\sum P(x_{t+1}|z_{t+1},x{1:t})P(z_{t+1}|x_{1:t})    \n",
   ]
  }
   ],
   "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
