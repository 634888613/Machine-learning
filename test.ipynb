{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.损失函数\n",
    "利用等式$y=3x+2$我造了一些伪数据，并给$x$添加了一些噪声数据，线性回归的目标即在只有$x,y$的情况下，求解出最优解：$w=[3,2]^T$；可以通过MSE（均方误差）来衡量$f(x)$与$y$的相近程度：  \n",
    "\n",
    "$$\n",
    "L(w)=\\sum_{i=1}^m(y_i-f(x_i))^2=\\sum_{i=1}^m(y_i-w^Tx_i^*)^2=(Y-X^*w)^T(Y-X^*w)\n",
    "$$  \n",
    "\n",
    "这里$m$表示样本量，本例中$m=100$，$x_i,y_i$表示第$i$个样本，$X^*\\in R^{m \\times (n+1)},Y\\in R^{m\\times 1}$，损失函数$L(w)$本质上是关于$w$的函数，通过求解最小的$L(w)$即可得到$w$的最优解：\n",
    "\n",
    "$$\n",
    "w^*=arg \\min_{w}L(w)\n",
    "$$\n",
    "\n",
    "\n",
    "#### 方法一：直接求闭式解\n",
    "\n",
    "而对$\\min L(w)$的求解很明显是一个凸问题（海瑟矩阵${X^*}^TX^*$正定），我们可以直接通过求解$\\frac{dL}{dw}=0$得到$w^*$，梯度推导如下：  \n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw}=-2\\sum_{i=1}^m(y_i-w^Tx_i^*)x_i^*=-2{X^*}^T(Y-X^*w)\\\\\n",
    "$$  \n",
    "令$\\frac{dL}{dw}=0$，可得：$w^*=({X^*}^TX^*)^{-1}{X^*}^TY$，实际情景中数据不一定能满足${X^*}^TX$是满秩（比如$m<n$的情况下，$w$的解有无数种），所以没法直接求逆，我们可以考虑用如下的方式求解：\n",
    "$$\n",
    "{X^*}^+=\\lim_{\\alpha\\rightarrow0}({X^*}^TX^*+\\alpha I)^{-1}{X^*}^T\n",
    "$$  \n",
    "\n",
    "上面的公式即是Moore-Penrose伪逆的定义，但实际求解更多是通过SVD的方式：  \n",
    "\n",
    "$$\n",
    "{X^*}^+=VD^+U^T\n",
    "$$  \n",
    "\n",
    "其中，$U,D,V$是矩阵$X^*$做奇异值分解（SVD）后得到的矩阵，对角矩阵$D$的伪逆$D^+$由其非零元素取倒数之后再转置得到，通过伪逆求解到的结果有如下优点：  \n",
    "\n",
    "（1）当$w$有解时，$w^*={X^*}^+Y$是所有解中欧几里得距离$||w||_2$最小的一个；    \n",
    "\n",
    "（2）当$w$无解时，通过伪逆得到的$w^*$是使得$X^*w^*$与$Y$的欧几里得距离$||X^*w^*-Y||_2$最小  \n",
    "\n",
    "#### 方法二：梯度下降求解\n",
    "\n",
    "但对于数据量很大的情况，求闭式解的方式会让内存很吃力，我们可以通过随机梯度下降法（SGD）对$w$进行更新，首先随机初始化$w$，然后使用如下的迭代公式对$w$进行迭代更新：  \n",
    "$$\n",
    "w:=w-\\eta\\frac{dL}{dw}\n",
    "$$  \n",
    "### 三.模型训练\n",
    "目前我们推导出了$w$的更新公式，接下来编码训练过程："
   ]
  },
 "nbformat": 4,
 "nbformat_minor": 2
}
