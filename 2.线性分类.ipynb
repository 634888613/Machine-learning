{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.模型结构\n",
    "线性回归算是回归任务中比较简单的一种模型，它的模型结构可以表示如下：   \n",
     "回顾
通过打开不同的特点转变成新的模型
线性：
属性非线性--特征转换（多项式回归）
全局非线性--线性分类（激活函数非线性）
系数非线性--神经网络
全局性--线性样条回归（分割样本空间），决策树
数据未加工--PCA，流形

线性分类
从线性回归出发，通过激活函数转换为0/1；从降维的角度，从P维数据变换为1维数据
硬分类：输出{0，1}，线性判别分析(fisher)，感知机
软分类：输出(0，1)，生成式模型(Gaussian Discriminant Analysis)；判别式模型(Logistic Regression)；

1.感知机  
想法：错误驱动
模型：f(x) = sign(w^t x), x;w R^p    sign(a) = 1 a>0; -1 a<0
策略：loss function
D:{被错误分类的样本}
L(w) = 错误分类数量；但不可导
可改为L(w) = 错位分类的点 sum y_i w^T x_i
算法：SGD，梯度下降迭代w

2.线性判别(fisher判别分析)
数据 X；Y；y_i {-1,1}
思想：在一个合适的投影方向进行投影（降维），类内差异小类间差异大
  类间：类间均值差值的平方
  类内：用方差表示
目标函数：类间/（类内）J(w) = (z_1-z_2)^2 / S_1 + S_2
argmax(w) J(w) = w^T (\bar x_c1 - \bar x_c2)(\bar x_c1 - \bar x_c2)^T w /(w_T (s_c1 + s_c2)w)
S为方差，c1 c2 为两个类别内的数据样本
J(w) = W^T S_b W / W^T S_w W
S_b 类间方差；S_w 类内方差
\partial J(W) / \partial w = 0

3.逻辑回归
概率判别模型，对P(Y|x)进行建模
逻辑回归使用MLE

4.高斯判别分析
概率生成模型，并不直接计算P(y|x)计算 \hat y = argmax(y=0/1) P(y|x) (= P(x|y)P(y))
而是比较 P(y=0|x) P(y=1|x) 
P(y|x) 后验 正比于 P(x|y)P(y)  似然*先验
先验：y服从伯努利分布；P(y=1) = \fai
后验：x|y 服从高斯分布：x|y=1 ~ N(miu_1, \sum) ； x|y=0 ~ N(miu_2, \sum)
使用MLE求参数，均值、方差、先验的分布均为参数(计算协方差矩阵时需要使用矩阵绝对值以及矩阵迹的偏导公式)
\partial tr(AB) / \partial A = B^T
\partial |A| / \partial A = |A|A^(-1)

5.朴素贝叶斯
思想：朴素贝叶斯假设；条件独立性假设；最简单的概率图模型（有向图）；给定y x_i 独立于 x_j(第i\j维的方向)
P(x|y) = \product P(x_j|y)
假设：x离散，服从categorial dist；x连续，Guassian dist
\hat y = argmax(y=0/1) P(y|x) (= P(x|y)P(y))
使用MLE直接进行计算
"
    "\n",
    "$$\n",
    "f(x)=w^Tx^*\n",
    "$$  \n",
    "这里$x^*=[x^T,1]^T$，$x\\in R^n$，所以$w\\in R^{n+1}$，$w$即是模型需要学习的参数，下面造一些伪数据进行演示："
   ]
  }
