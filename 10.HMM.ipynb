{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近似推断/精确推断，其中近似推断重的随即推断中最著名的即为MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.背景知识  \n",
     "概率图模型，有向图-贝叶斯网络；无向图-马尔可夫随机场；添加时间轴，变成动态模型：HMM;Kalman Filter；Partile Filter.对于因变量state，如果离散为HMM，连续且线性，Kalman Filter;非线性， Parlicle Filter      \n",
     "\\lambda=(\\pi,A,B),\\pi为初始概率分布，A为状态转移矩阵，B为发射矩阵，观测变量用o_t表示->值域V={v_1,v_2....v_N}，状态变量用i_t表示->值域Q={q_1,q_2...q_M},A[a_{ij}],从状态i转移到状态j，发射矩阵B=[b_j(k)],b_j(k)=P(O_t=V_t|i_t=q_j),从状态到观测变量的关系       \n",
     "两个假设，1）齐次markov假设，无后效性；2）观测独立假设",
     "HMM解决的三个问题，1）求知，已知\\lambda，P(O|\\lambda)，使用前后向算法；2）learning，参数计算问题，使似然最大，使用EM算法；3）Decoding，I=argmaxP(I|O)，找到序列I，预测问题，已知观测序列，预测下一个时刻的隐状态+滤波问题已知观测变量，求t时刻的隐状态"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.HMM-Evaluation问题  \n",
    "利用\\lambda求P(O|\\lambda)",
     "P(O|\\lambda)=\\sum P(I,O|\\lambda)=\\sum P(O|I,\\lambda)P(I|\\lambda),复杂度为O(N^T)    \n",
     "前向算法：\\alpha_t(i)=P(o_1,...o_t,i_t=q_i|\\lambda),\\alpha_T(i)=P(O,i_t=q_i|\\lambda)   \n",
     "P(O|\\lambda)=\\sum_{i=1}^N P(O,i_t=q_i|\\lambda)=\\sum \\alpha_T(i)",
     "从观测序列向后推\\alpha_{t+1}(j)=P(o_1,...o_t,o_{t+1},i_{t+1}=q_j|\\lambda)=\\sum P(o...,i_{t+1},i_t)=\\sum P(O_{t+1}|i_{t+1}=q_j) P(...))(无后效性假设)，把i_{t+1}提出来=\\sum_{i=1}^N b_j(O_{t+1})a_{ij}\\alpha_t(i)，即为前向递归公式",
     "后向算法：引入\\beta_t(i)=P(o_{t+1},...,o_T|i_t=q_i,\\lambda),从后往前推，\\beta_1(i)=P(O_2,...,O_T|i_1=q_i,\\lambda)",
     "P(O|\\lambda)=P(o_1,...o_T|\\lambda)=\\sum P(o_1,...o_T|i_1=q_i)P(i_1=q_i)(->\\pi_i)=\\sum b_i(o_1)\\beta_1(i)",
     "\\beta_t(i)=\\sum b_j(O_{t+1})a_{ij}\\beta_{t+1}(j)",
     "\\beta_T->\\beta_1->P(O|\\lambda)"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.HMM-Learning问题 \n",
    "使用EM算法，\\theta^{t+1}=argmax \\int_z log p(x,z|\\theta)p(z|x,\\theta^{t})dz,x是观测数据，z是隐变量，\\theta是参数     \n",
     "Q(\\lambda,\\lambda^(t))=\\sum(隐变量) log P(O,I|\\lambda)P(O,I|\\lambda^t)",
     "\\pi^{t+1}=argmax \\sum [log \\pi_i P(O,i_1=q_i|\\lambda^t)] s.t.\\sum \\pi_i=1",
     "\\pi_i^t=\\frac{P(O,i_1=q_i|\\lambda^t)}{P(O|\\lambda^t)}",
     "A,B的计算的思想与计算\\pi的思想相同，PS：这个就是Baum Welch算法，但其实就是EM"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.HMM-Decoding问题 \n",
    "\\hat{I}=argmax_I P(I|I,\\lambda),可以理解为一个动态规划问题,Viterbi算法,   \n",
     "\\delta_t(i)=max P(o_1...o_t,i_1...i_t=q_i),接下来需要寻找递归式   \n",
     "\\delta_{t+1}(j)=max P(o_1,o_2,...o_{t+1},i_1,...i_{t+1}=q_j)=max \\delta_t(i)a_{ij}b_j(O_{t+1})  \n",
     "\\phi_{t+1}(j)=argmax_{1<=i<=N} \\delta_t(i)a_{ij},寻找从哪来的，记录最短路径序列，进而排成一个最短路径"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五.HMM \n",
    "\\   \n",
     
   ]
  }
   ],
   "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
